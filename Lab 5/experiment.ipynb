{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "3ec50932",
      "cell_type": "markdown",
      "source": "Assignment 5",
      "metadata": {}
    },
    {
      "id": "cb0613b4",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Step 1: Importing and Merging Data\ntry:\n    # Use try/except to handle potential file not found errors\n    churn_data = pd.read_csv(\"churn_data.csv\")\n    customer_data = pd.read_csv(\"customer_data.csv\")\n    internet_data = pd.read_csv(\"internet_data.csv\")\n    \n    df_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')\n    telecom = pd.merge(df_1, internet_data, how='inner', on='customerID')\n    \n    print(\"Data loaded and merged successfully.\")\n    print(f\"Dataset shape: {telecom.shape}\")\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n    # Since this is a code sample, we'll continue assuming data is loaded\n\n# Step 2: Initial Data Inspection\nprint(\"\\n--- First 5 rows of data ---\")\nprint(telecom.head())\n\nprint(\"\\n--- Data Types ---\")\nprint(telecom.dtypes)\n\n# Step 3: Data Preparation\n# First, save customerID for later reference\ncustomer_ids = telecom['customerID'].copy()\n\n# Convert binary categorical variables to numeric\nbinary_vars = ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']\nfor var in binary_vars:\n    if var in telecom.columns:\n        telecom[var] = telecom[var].map({'Yes': 1, 'No': 0})\n        # Ensure proper numeric type\n        telecom[var] = pd.to_numeric(telecom[var], errors='coerce')\n\n# Handle 'TotalCharges' column\nif 'TotalCharges' in telecom.columns:\n    # First, check if it's already numeric\n    if not pd.api.types.is_numeric_dtype(telecom['TotalCharges']):\n        telecom['TotalCharges'] = pd.to_numeric(telecom['TotalCharges'], errors='coerce')\n    \n    # Drop rows with missing TotalCharges\n    telecom = telecom.dropna(subset=['TotalCharges'])\n    print(f\"Dropped {len(customer_ids) - len(telecom)} rows with missing TotalCharges\")\n\n# Create dummy variables for categorical columns\ncategorical_cols = [\n    'Contract', 'PaymentMethod', 'gender', 'InternetService',\n    'MultipleLines', 'OnlineSecurity', 'OnlineBackup', \n    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'\n]\n\n# Filter to only include columns that actually exist in the dataset\ncategorical_cols = [col for col in categorical_cols if col in telecom.columns]\n\n# Create dummies for all categorical columns at once\nif categorical_cols:\n    dummies = pd.get_dummies(telecom[categorical_cols], drop_first=True)\n    telecom = pd.concat([telecom, dummies], axis=1)\n    \n    # Drop original categorical columns\n    telecom = telecom.drop(categorical_cols, axis=1)\n\n# Drop customerID before modeling\nif 'customerID' in telecom.columns:\n    telecom = telecom.drop('customerID', axis=1)\n\n# Step 4: Split data into features and target\nif 'Churn' in telecom.columns:\n    X = telecom.drop('Churn', axis=1)\n    y = telecom['Churn']\n    \n    # Ensure y is a proper numeric vector\n    y = pd.to_numeric(y, errors='coerce')\n    y = y.fillna(0).astype(int)  # Fill any NAs and convert to int\nelse:\n    print(\"Error: 'Churn' column not found in the dataset.\")\n    # For this example, we'll just continue\n\n# Step 5: Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)\n\n# Step 6: Feature Scaling (only for numeric columns)\nnumeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\nif 'tenure' in numeric_cols and 'MonthlyCharges' in numeric_cols and 'TotalCharges' in numeric_cols:\n    scaler = StandardScaler()\n    X_train[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.fit_transform(X_train[['tenure', 'MonthlyCharges', 'TotalCharges']])\n    \n    # Apply the same transformation to test data\n    X_test[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.transform(X_test[['tenure', 'MonthlyCharges', 'TotalCharges']])\n\n# Step 7: Final Data Preparation for Modeling\n# Convert all feature columns to numeric explicitly\nfor col in X_train.columns:\n    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n\n# Fill any remaining NaN values with the mean of each column\nX_train = X_train.fillna(X_train.mean())\nX_test = X_test.fillna(X_test.mean())\n\n# Reset indices to ensure alignment\nX_train = X_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n\n# Step 8: Check data readiness\nprint(\"\\n--- Data Preparation Summary ---\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_train has NaN values: {X_train.isna().any().any()}\")\nprint(f\"y_train has NaN values: {y_train.isna().any()}\")\nprint(f\"X_train dtypes: {X_train.dtypes.value_counts()}\")\n\n# Step 9: Convert to NumPy arrays (which works better with statsmodels)\nX_train_np = X_train.to_numpy()\ny_train_np = y_train.to_numpy()\n\nprint(f\"X_train_np dtype: {X_train_np.dtype}\")\nprint(f\"y_train_np dtype: {y_train_np.dtype}\")\n\n# Step 10: Build the model\nprint(\"\\n--- Model Building ---\")\n\n# Add constant (intercept term)\nX_train_const = sm.add_constant(X_train_np)\n\n# Method 1: Try statsmodels GLM\ntry:\n    print(\"Attempting statsmodels GLM...\")\n    \n    # Force data to be float64\n    X_train_const = X_train_const.astype(np.float64)\n    y_train_np = y_train_np.astype(np.float64)\n    \n    logm1 = sm.GLM(y_train_np, X_train_const, family=sm.families.Binomial())\n    result = logm1.fit()\n    \n    print(\"GLM model built successfully!\")\n    print(\"\\n--- Model Summary ---\")\n    print(result.summary())\n    \nexcept Exception as e:\n    print(f\"Error with statsmodels GLM: {e}\")\n    \n    # Method 2: Try statsmodels Logit as an alternative\n    try:\n        print(\"\\nAttempting statsmodels Logit...\")\n        logit_model = sm.Logit(y_train_np, X_train_const)\n        result = logit_model.fit()\n        \n        print(\"Logit model built successfully!\")\n        print(\"\\n--- Model Summary ---\")\n        print(result.summary())\n        \n    except Exception as e2:\n        print(f\"Error with statsmodels Logit: {e2}\")\n        \n        # Method 3: Fallback to sklearn's LogisticRegression\n        try:\n            print(\"\\nFalling back to sklearn's LogisticRegression...\")\n            lr = LogisticRegression(max_iter=1000, random_state=100)\n            lr.fit(X_train_np, y_train_np)\n            \n            print(\"sklearn LogisticRegression model built successfully!\")\n            \n            # Calculate feature importance\n            feature_importance = pd.DataFrame({\n                'Feature': X_train.columns,\n                'Coefficient': lr.coef_[0]\n            })\n            \n            print(\"\\n--- Top 10 Important Features ---\")\n            print(feature_importance.sort_values('Coefficient', ascending=False).head(10))\n            \n            # Calculate and print model accuracy\n            train_accuracy = lr.score(X_train_np, y_train_np)\n            test_accuracy = lr.score(X_test.to_numpy(), y_test.to_numpy())\n            \n            print(f\"\\nTraining accuracy: {train_accuracy:.4f}\")\n            print(f\"Testing accuracy: {test_accuracy:.4f}\")\n            \n        except Exception as e3:\n            print(f\"Error with sklearn LogisticRegression: {e3}\")\n            print(\"All modeling approaches failed. Please check your data preprocessing steps.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Data loaded and merged successfully.\nDataset shape: (7043, 21)\n\n--- First 5 rows of data ---\n   customerID  tenure PhoneService        Contract PaperlessBilling  \\\n0  7590-VHVEG       1           No  Month-to-month              Yes   \n1  5575-GNVDE      34          Yes        One year               No   \n2  3668-QPYBK       2          Yes  Month-to-month              Yes   \n3  7795-CFOCW      45           No        One year               No   \n4  9237-HQITU       2          Yes  Month-to-month              Yes   \n\n               PaymentMethod  MonthlyCharges TotalCharges Churn  gender  ...  \\\n0           Electronic check           29.85        29.85    No  Female  ...   \n1               Mailed check           56.95       1889.5    No    Male  ...   \n2               Mailed check           53.85       108.15   Yes    Male  ...   \n3  Bank transfer (automatic)           42.30      1840.75    No    Male  ...   \n4           Electronic check           70.70       151.65   Yes  Female  ...   \n\n   Partner Dependents     MultipleLines InternetService OnlineSecurity  \\\n0      Yes         No  No phone service             DSL             No   \n1       No         No                No             DSL            Yes   \n2       No         No                No             DSL            Yes   \n3       No         No  No phone service             DSL            Yes   \n4       No         No                No     Fiber optic             No   \n\n  OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies  \n0          Yes               No          No          No              No  \n1           No              Yes          No          No              No  \n2          Yes               No          No          No              No  \n3           No              Yes         Yes          No              No  \n4           No               No          No          No              No  \n\n[5 rows x 21 columns]\n\n--- Data Types ---\ncustomerID           object\ntenure                int64\nPhoneService         object\nContract             object\nPaperlessBilling     object\nPaymentMethod        object\nMonthlyCharges      float64\nTotalCharges         object\nChurn                object\ngender               object\nSeniorCitizen         int64\nPartner              object\nDependents           object\nMultipleLines        object\nInternetService      object\nOnlineSecurity       object\nOnlineBackup         object\nDeviceProtection     object\nTechSupport          object\nStreamingTV          object\nStreamingMovies      object\ndtype: object\nDropped 11 rows with missing TotalCharges\n\n--- Data Preparation Summary ---\nX_train shape: (4922, 30)\ny_train shape: (4922,)\nX_train has NaN values: False\ny_train has NaN values: False\nX_train dtypes: bool       22\nint64       5\nfloat64     3\nName: count, dtype: int64\nX_train_np dtype: object\ny_train_np dtype: int32\n\n--- Model Building ---\nAttempting statsmodels GLM...\nGLM model built successfully!\n\n--- Model Summary ---\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 4922\nModel:                            GLM   Df Residuals:                     4898\nModel Family:                Binomial   Df Model:                           23\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -2005.2\nDate:                Mon, 12 May 2025   Deviance:                       4010.3\nTime:                        12:00:31   Pearson chi2:                 5.94e+03\nNo. Iterations:                   100   Pseudo R-squ. (CS):             0.2842\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst      -3.803e+12   8.04e+12     -0.473      0.636   -1.96e+13     1.2e+13\nx1            -1.5146      0.188     -8.042      0.000      -1.884      -1.145\nx2          3.803e+12   8.04e+12      0.473      0.636    -1.2e+13    1.96e+13\nx3             0.3255      0.090      3.629      0.000       0.150       0.501\nx4            -2.1819      1.154     -1.890      0.059      -4.444       0.081\nx5             0.7309      0.197      3.708      0.000       0.345       1.117\nx6             0.3988      0.101      3.935      0.000       0.200       0.597\nx7             0.0376      0.093      0.403      0.687      -0.145       0.221\nx8            -0.1426      0.107     -1.335      0.182      -0.352       0.067\nx9            -0.6589      0.128     -5.154      0.000      -0.909      -0.408\nx10           -1.2450      0.209     -5.944      0.000      -1.656      -0.834\nx11           -0.2585      0.136     -1.900      0.057      -0.525       0.008\nx12            0.1601      0.112      1.427      0.153      -0.060       0.380\nx13           -0.2521      0.137     -1.840      0.066      -0.521       0.016\nx14           -0.0336      0.078     -0.430      0.667      -0.187       0.119\nx15            2.5132      0.962      2.612      0.009       0.627       4.399\nx16           -0.3984      0.140     -2.853      0.004      -0.672      -0.125\nx17         3.803e+12   8.04e+12      0.473      0.636    -1.2e+13    1.96e+13\nx18            0.5638      0.213      2.646      0.008       0.146       0.981\nx19           -0.3977      0.140     -2.848      0.004      -0.671      -0.124\nx20           -0.0252      0.215     -0.117      0.907      -0.447       0.396\nx21           -0.3980      0.140     -2.851      0.004      -0.672      -0.124\nx22            0.1759      0.211      0.835      0.404      -0.237       0.589\nx23           -0.3980      0.140     -2.851      0.004      -0.672      -0.124\nx24            0.3241      0.214      1.513      0.130      -0.096       0.744\nx25           -0.3980      0.140     -2.851      0.004      -0.672      -0.124\nx26           -0.0296      0.215     -0.137      0.891      -0.451       0.392\nx27           -0.3980      0.140     -2.851      0.004      -0.672      -0.124\nx28            0.9612      0.394      2.438      0.015       0.189       1.734\nx29           -0.3980      0.140     -2.851      0.004      -0.672      -0.124\nx30            0.8502      0.394      2.158      0.031       0.078       1.623\n==============================================================================\n"
        }
      ],
      "execution_count": 25
    },
    {
      "id": "9288c4ac-aba6-4f59-8282-9473e3f3fffa",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}